{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI data analysis with `cloudknot`\n",
    "\n",
    "This example demonstrates analysis of MRI data using software that depends on the scipy stack, \n",
    "as well as on smaller open-source software projects\n",
    "\n",
    "The data for each participant in the study is non-trivial in size (about 140 MB per subject),\n",
    "and complexity (4D arrays representing MRI measurements in every location in the brain with \n",
    "diffusion gradients in multiple directions). The analysis requires non-trivial computations \n",
    "(e.g., fitting a linear model over directions in every spatial location). \n",
    "\n",
    "The data we will use is publicly accessible through a URL pointing to the Stanford Data Repository\n",
    "We will pull down each participant's data using a URL that the function constructs based on its \n",
    "input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cloudknot as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_subject_data(sub):\n",
    "    \"\"\" \n",
    "    Perform non-trivial analysis of non-trivial data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    sub : str\n",
    "        The ID string forone of the subjects in this data-set. \n",
    "        One of  {'SUB1', 'SUB2', 'SUB3', 'SUB4', 'SUB5', 'SUB6'}\n",
    "    \n",
    "    \"\"\"\n",
    "    import os\n",
    "    import os.path as op\n",
    "    import requests\n",
    "\n",
    "    # Helper function to download files and save them:\n",
    "    def download_file(url, fname):\n",
    "        if not op.exists(fname):\n",
    "            r = requests.get(url)\n",
    "            with open(fname , 'wb') as fd:\n",
    "                for chunk in r.iter_content(chunk_size=128):\n",
    "                    fd.write(chunk)\n",
    "    \n",
    "    # Create the folder structure AFQ expects:\n",
    "    base_folder = op.join(op.expanduser('~'), 'data')\n",
    "    if not op.exists(base_folder):\n",
    "        os.mkdir(base_folder)\n",
    "        os.mkdir(op.join(base_folder, 'sub-01'))\n",
    "        os.mkdir(op.join(base_folder, 'sub-01', 'sess-01'))\n",
    "        anat_folder = op.join(base_folder, 'sub-01', 'sess-01', 'anat')\n",
    "        os.mkdir(anat_folder)\n",
    "        dwi_folder = op.join(base_folder, 'sub-01', 'sess-01', 'dwi')\n",
    "        os.mkdir(dwi_folder)\n",
    "\n",
    "    # Grab the data from the Stanford Data Repository:\n",
    "    data_url = 'https://stacks.stanford.edu/file/druid:rt034xr8593/'\n",
    "    sub_data = data_url + sub\n",
    "    fbvals_url = sub_data + \"_1.bvals\"\n",
    "    fbvecs_url = sub_data + \"_1.bvecs\"\n",
    "    fnii_url = sub_data + \"_1.nii.gz\"\n",
    "    \n",
    "    # Download and save into the designated location:\n",
    "    download_file(fnii_url, op.join(dwi_folder, 'dwi.nii.gz'))\n",
    "    download_file(fbvals_url, op.join(dwi_folder, 'dwi.bvals'))\n",
    "    download_file(fbvecs_url, op.join(dwi_folder, 'dwi.bvecs'))\n",
    "\n",
    "    # AFQ knows how to find the files:    \n",
    "    from AFQ.api import AFQ\n",
    "    my_afq = AFQ(preproc_path=base_folder, sub_prefix='sub')\n",
    "    \n",
    "    # This triggers the analysis:\n",
    "    fa_file = my_afq.dti_fa[0]\n",
    "    \n",
    "    # Upload to S3:\n",
    "    client = boto3.resource('s3')\n",
    "    bucket_name = 'escience.washington.edu.public'\n",
    "    b = client.Bucket(bucket_name)\n",
    "    b.upload_file(fa_file, '%s_FA.nii.gz' % sub)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    knot = ck.Knot(name='write_to_s3_bucket',\n",
    "                   func=process_subject_data,\n",
    "                   pars_policies=('AmazonS3FullAccess',))\n",
    "except ValueError:    \n",
    "    # If you previously created this knot but didn't clobber it, then just supply\n",
    "    # the name in order to retrieve the knot info from the cloudknot config file\n",
    "    knot = ck.Knot(name='test_s3_knot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_futures = knot.map(['SUB1', 'SUB2', 'SUB3', 'SUB4', 'SUB5', 'SUB6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knot.view_jobs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
